# GUI ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ íŒŒì¼

import streamlit as st
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from typing import TypedDict, Annotated, List
import operator
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.tools import tool


# --- 1. ì„¤ì • ë° ì—ì´ì „íŠ¸ ë¡œì§ ---

load_dotenv()
MODEL_NAME = "gemini-2.5-flash"

@tool
def web_search(query: str) -> str:
    """
    ìµœì‹  ì •ë³´, íŠ¹ì • ì¸ë¬¼, ì¥ì†Œ, ì´ë²¤íŠ¸, ê¸°ìˆ  ìš©ì–´(ì˜ˆ: Gemini 2.5) ë˜ëŠ” ì‹¤ì‹œê°„ ì •ë³´(ì˜ˆ: ë‚ ì”¨, ë‰´ìŠ¤)ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ AIì˜ ë‚´ë¶€ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€í•˜ê¸° ì–´ë µë‹¤ê³  íŒë‹¨ë  ë•Œ, ë°˜ë“œì‹œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ APIë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ì§€ê¸ˆì€ ê°€ì§œ ê²°ê³¼ë¬¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    st.sidebar.info(f"ğŸ” ì›¹ ê²€ìƒ‰ ìˆ˜í–‰: {query}") # UIì— ê²€ìƒ‰ ê³¼ì •ì„ í‘œì‹œ
    if "gemini-2.5" in query.lower():
        return "Gemini 2.5ëŠ” Googleì˜ ìµœì‹  ê³ ì„±ëŠ¥ ëª¨ë¸ë¡œ, Flashì™€ Pro ë²„ì „ì´ ì¡´ì¬í•©ë‹ˆë‹¤."
    return f"'{query}'ì— ëŒ€í•œ ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."

tools = [web_search]

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]

model = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.7)
model_with_tools = model.bind_tools(tools)

def call_model(state: AgentState):
    response = model_with_tools.invoke(state['messages'])
    return {"messages": [response]}

tool_node = ToolNode(tools)

def should_continue(state: AgentState) -> str:
    if state['messages'][-1].tool_calls:
        return "call_tool"
    return "__end__"

workflow = StateGraph(AgentState)
workflow.add_node("llm", call_model)
workflow.add_node("call_tool", tool_node)
workflow.set_entry_point("llm")
workflow.add_conditional_edges("llm", should_continue)
workflow.add_edge("call_tool", "llm")
app = workflow.compile()


# --- 2. Streamlit UI êµ¬í˜„ ---

st.set_page_config(page_title="Context Keeper", page_icon="ğŸ§ ")
st.title("ğŸ§  Context Keeper")

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìœ ëŠ¥í•˜ê³  ì ê·¹ì ì¸ AI ë¹„ì„œ 'Context Keeper'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìµœëŒ€í•œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
2. ëª¨ë¥´ëŠ” ì •ë³´ë‚˜ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´, ì£¼ì €í•˜ì§€ ë§ê³  ë‹¹ì‹ ì´ ê°€ì§„ 'web_search' ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. ëŒ€í™”ì˜ ì „ì²´ ë§¥ë½ì„ í•­ìƒ ê¸°ì–µí•˜ê³ , ì‚¬ìš©ìê°€ ëª¨í˜¸í•˜ê²Œ ë§í•˜ë”ë¼ë„ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤."""

# ** Streamlitì˜ ì„¸ì…˜ ìƒíƒœ(Session State)ë¥¼ ì´ìš©í•œ ëŒ€í™” ê¸°ë¡ ìœ ì§€ **
# st.session_stateëŠ” ì›¹í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ ë˜ì–´ë„ ê°’ì„ ìœ ì§€í•´ì£¼ëŠ” ë§ˆë²• ê°™ì€ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT)]

# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— í‘œì‹œ
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            # AIMessageì˜ contentê°€ ë³µì¡í•œ êµ¬ì¡°ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            content = message.content
            if isinstance(content, list) and content and isinstance(content[0], dict):
                st.markdown(content[0].get('text', ''))
            else:
                st.markdown(content)

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ëŠ” ì±„íŒ… ì…ë ¥ì°½
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # ë¡œë”© ìŠ¤í”¼ë„ˆ í‘œì‹œ
    with st.spinner("ìƒê° ì¤‘..."):
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        inputs = {"messages": st.session_state.messages}
        final_state = app.invoke(inputs)
        
        # ì‹¤í–‰ í›„ì˜ ì „ì²´ ë©”ì‹œì§€ ê¸°ë¡ìœ¼ë¡œ ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸
        st.session_state.messages = final_state['messages']
        
        # ë§ˆì§€ë§‰ AI ì‘ë‹µë§Œ ê°€ì ¸ì™€ì„œ í™”ë©´ì— ìƒˆë¡œ í‘œì‹œ
        ai_response_message = st.session_state.messages[-1]

        with st.chat_message("assistant"):
            content = ai_response_message.content
            if isinstance(content, list) and content and isinstance(content[0], dict):
                st.markdown(content[0].get('text', ''))
            else:
                st.markdown(content)