# GUI ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ íŒŒì¼

import streamlit as st
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from typing import TypedDict, Annotated, List
import operator
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage
from langchain_core.tools import tool
from langchain_tavily import TavilySearch
from google.api_core import exceptions


# --- 1. ì„¤ì • ë° ì—ì´ì „íŠ¸ ë¡œì§ ---

load_dotenv()
MODEL_NAME = "gemini-2.5-flash"

search_tool = TavilySearch(max_results=3)
search_tool.name = "web_search" # ê¸°ë³¸ ë„êµ¬ ì´ë¦„ì€ 'tavily_search'
tools = [search_tool]

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]

model = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.85)
model_with_tools = model.bind_tools(tools)

def call_model(state: AgentState):
    response = model_with_tools.invoke(state['messages'])
    return {"messages": [response]}

tool_node = ToolNode(tools)

def should_continue(state: AgentState) -> str:
    last_message = state["messages"][-1]
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        return "call_tool"
    return "__end__"

workflow = StateGraph(AgentState)
workflow.add_node("llm", call_model)
workflow.add_node("call_tool", tool_node)
workflow.set_entry_point("llm")
workflow.add_conditional_edges("llm", should_continue)
workflow.add_edge("call_tool", "llm")
app = workflow.compile()


# --- 2. LangGraph ìŠ¤íŠ¸ë¦¼ì„ ì†Œë¹„í•˜ê³ , í…ìŠ¤íŠ¸ ì²­í¬ë§Œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

def get_content_from_message(message: BaseMessage) -> str:
    """ëª¨ë“  ì¢…ë¥˜ì˜ ë©”ì‹œì§€ ê°ì²´ì—ì„œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not isinstance(message, AIMessage):
        return message.content
    
    content = message.content
    if isinstance(content, list) and content and isinstance(content[0], dict):
        return content[0].get('text', '')
    return str(content) # ë¬¸ìì—´ì´ê±°ë‚˜ ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬

def run_agent(user_input: list):
    inputs = {"messages": user_input}
    
    # app.stream()ì€ ë³µì¡í•œ ì´ë²¤íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    for event in app.stream(inputs, stream_mode="values"):
        # ê° ì´ë²¤íŠ¸ì—ì„œ 'messages' í‚¤ì˜ ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        message_chunk_list = event.get("messages", [])
        if message_chunk_list:
            # messagesëŠ” í•­ìƒ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ë§ˆì§€ë§‰ í•­ëª©ì„ í™•ì¸í•©ë‹ˆë‹¤.
            last_message_chunk = message_chunk_list[-1]
            if isinstance(last_message_chunk, AIMessage):
                # AIMessage ì²­í¬ì˜ contentë§Œ st.write_streamìœ¼ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
                yield last_message_chunk.content


# --- 3. Streamlit UI êµ¬í˜„ ---

st.set_page_config(page_title="Context Keeper", page_icon="ğŸ§ ")
st.title("ğŸ§  Context Keeper")
st.sidebar.title("Agent Status")
st.sidebar.markdown("ì—ì´ì „íŠ¸ì˜ ìƒê° ê³¼ì •ì´ë‚˜ ë„êµ¬ ì‚¬ìš© ë‚´ì—­ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.")

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ìœ ëŠ¥í•˜ê³  ì ê·¹ì ì¸ AI ë¹„ì„œ 'Context Keeper'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìµœëŒ€í•œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤.
2. ëª¨ë¥´ëŠ” ì •ë³´ë‚˜ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´, ì£¼ì €í•˜ì§€ ë§ê³  ë‹¹ì‹ ì´ ê°€ì§„ 'web_search' ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. ëŒ€í™”ì˜ ì „ì²´ ë§¥ë½ì„ í•­ìƒ ê¸°ì–µí•˜ê³ , ì‚¬ìš©ìê°€ ëª¨í˜¸í•˜ê²Œ ë§í•˜ë”ë¼ë„ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤."""

# ** Streamlitì˜ ì„¸ì…˜ ìƒíƒœ(Session State)ë¥¼ ì´ìš©í•œ ëŒ€í™” ê¸°ë¡ ìœ ì§€ **
# st.session_stateëŠ” ì›¹í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ ë˜ì–´ë„ ê°’ì„ ìœ ì§€í•´ì£¼ëŠ” ë§ˆë²• ê°™ì€ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state.messages = [SystemMessage(content=SYSTEM_PROMPT)]

# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ í•¨ìˆ˜
def display_messages():
    for message in st.session_state.messages:
        if isinstance(message, HumanMessage):
            with st.chat_message("user"):
                st.markdown(message.content)
        elif isinstance(message, AIMessage):
            with st.chat_message("assistant"):
                # AIMessageì˜ contentê°€ ë³µì¡í•œ êµ¬ì¡°ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                content = message.content
                if isinstance(content, list) and content and isinstance(content[0], dict):
                    st.markdown(content[0].get('text', ''))
                else:
                    st.markdown(content)
                    
display_messages()

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ëŠ” ì±„íŒ… ì…ë ¥ì°½
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì„ ê¸°ë¡í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            with st.spinner("ìƒê° ì¤‘..."):
                final_state = app.invoke({"messages": st.session_state.messages})
            final_ai_message = final_state['messages'][-1]
            
            # í–‰ë™ ë¶„ê¸°
            # Case A: ë§Œì•½ ì²« í–‰ë™ì´ 'ë„êµ¬ í˜¸ì¶œ'ì´ë¼ë©´
            if final_ai_message.tool_calls:
                tool_call = final_ai_message.tool_calls[0]
                st.sidebar.info(f"{tool_call['name']} í˜¸ì¶œ\n- ê²€ìƒ‰ì–´: {tool_call['args']['query']}")
                
                with st.spinner("ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„± ì¤‘..."):
                    response_stream = model.stream(final_state['messages'])
                    full_response = st.write_stream(
                        (chunk.content for chunk in response_stream if isinstance(chunk, AIMessage))
                    )
            
            # Case B: ë„êµ¬ ì‚¬ìš©x
            else:
                # ê°€ì§œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                full_response = st.write_stream(
                    (char for char in final_ai_message.content)
                )
            
            st.session_state.messages = final_state['messages']
            
        except exceptions.ServiceUnavailable as e:
            st.error("ëª¨ë¸ ì„œë²„ê°€ ì¼ì‹œì ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")